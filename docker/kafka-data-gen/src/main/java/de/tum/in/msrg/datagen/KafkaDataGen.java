package de.tum.in.msrg.datagen;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import de.tum.in.msrg.datagen.adapter.Adapter;
import de.tum.in.msrg.datagen.adapter.HDFSAdapter;
import de.tum.in.msrg.datagen.adapter.KafkaAdapter;
import de.tum.in.msrg.datamodel.ClickEvent;
import de.tum.in.msrg.datamodel.UpdateEvent;
import io.prometheus.client.Counter;
import io.prometheus.client.exporter.HTTPServer;
import org.apache.commons.cli.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.net.URISyntaxException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.Duration;
import java.time.Instant;
import java.time.temporal.ChronoUnit;
import java.util.*;

/**
 * Hello world!
 *
 */
public class KafkaDataGen
{
    public static final Duration WINDOW_SIZE = Duration.of(60, ChronoUnit.SECONDS );
    public static final int EVENTS_PER_WINDOW = 5;
    private static final List<String> pages = Arrays.asList("/help", "/index", "/shop", "/jobs", "/about", "/news");
    //this calculation is only accurate as long as pages.size() * EVENTS_PER_WINDOW divides the
    //window size
    public static final long DELAY = WINDOW_SIZE.toMillis() / pages.size() / EVENTS_PER_WINDOW;

    private final Adapter adapter;
    private final long delay;
    private final long delayLength;

    private final ObjectMapper objectMapper = new ObjectMapper();

    private static final Logger LOGGER = LogManager.getLogger(KafkaDataGen.class);


    public KafkaDataGen(Adapter adapter, long delay, long delayLength){
        this.adapter = adapter;
        this.delay = delay;
        this.delayLength = delayLength;
    }


    public static void main( String[] args ) throws ParseException, IOException {

        Options cliOptns = KafkaDataGen.createCLI();
        DefaultParser parser = new DefaultParser();
        CommandLine cmdLine = parser.parse(cliOptns, args);

        String bootstrap = cmdLine.getOptionValue("kafka", "kafka:9092");
        LOGGER.info(String.format("Kafka bootstrap server: %s", bootstrap));

        String hdfsUri = cmdLine.getOptionValue("hdfs", null);
        LOGGER.info(String.format("HDFS Uri: %s", hdfsUri));

//        String topic = cmdLine.getOptionValue("topic","input");
        long delay = Long.parseLong(cmdLine.getOptionValue("delay", "1000"));
        LOGGER.info(String.format("Thread sleep after %d records", delay));

        long delayLength = Long.parseLong(cmdLine.getOptionValue("length", "1"));
        LOGGER.info(String.format("Thread sleep for %d ms", delayLength));

        Adapter adapter = createAdapter(bootstrap, hdfsUri);
        KafkaDataGen dataGen = new KafkaDataGen(adapter, delay, delayLength);

//        Path rootReportFolder = Paths.get("/reports", Instant.now().toString());


        LOGGER.info("Creating prom server at port 52923...");
        try (
                HTTPServer promServer = new HTTPServer(52923);
        ){
            LOGGER.info("Creating counters...");
            Counter recordsCounter = Counter.build("de_tum_in_msrg_datagen_records_total", "Total number of messages generated by the generator").labelNames("key").register();
            Counter idRolloverCounter = Counter.build("de_tum_in_msrg_datagen_id_rollover_total", "Total number of times ID roll overed").register();

            ClickIterator clickIterator = new ClickIterator();

            long counter = 0L;
            // Update the pages half way the window
            long nextUpdate = (WINDOW_SIZE.toMillis()/2);

            LOGGER.info("Producing records...");
            while (true){
                ClickEvent clickEvent = clickIterator.next();
                if (clickEvent.getTimestamp().getTime() > nextUpdate){
                    updatePages(clickEvent, dataGen.adapter, dataGen.objectMapper, clickIterator);
                    nextUpdate += WINDOW_SIZE.toMillis();
                }

                dataGen.adapter.send("click", clickEvent.getPage(), Long.toString(clickEvent.getId()), dataGen.objectMapper.writeValueAsString(clickEvent));


                counter++;


                recordsCounter.labels(clickEvent.getPage()).inc();
                if (clickEvent.getId() == Long.MIN_VALUE){
                    idRolloverCounter.inc();
                }

                if (counter == dataGen.delay){
                    Thread.sleep(delayLength);
                    counter = 0;
                    dataGen.adapter.flush();
                }


            }

        } catch (InterruptedException e) {
            e.printStackTrace();
        }


    }

    private static Adapter createAdapter(String kafkaBootstrap, String hdfsUri){
        Adapter adapter = null;

        if (hdfsUri == null){
            // Create Kafka adapter
            Properties kafkaProperties = getKafkaProps(kafkaBootstrap);
            LOGGER.info(String.format("Creating Kafka adapter with properties: %s", kafkaProperties.toString()));

            adapter = new KafkaAdapter(kafkaProperties);
        } else {
            // crete HDFS adapter
            try {
                Configuration configuration = getHdfsConf(hdfsUri);
                LOGGER.info(String.format("Creating HDFS adapter with configuration: %s", configuration.toString()));

                adapter = new HDFSAdapter(configuration, hdfsUri);
            } catch (IOException e) {
                e.printStackTrace();
            } catch (URISyntaxException e) {
                e.printStackTrace();
            }
        }

        return adapter;
    }

    protected static void updatePages(ClickEvent clickEvent, Adapter adapter, ObjectMapper objectMapper, ClickIterator clickIterator) throws JsonProcessingException {
        for (String page : pages){
            clickIterator.id++;
            UpdateEvent updateEvent = new UpdateEvent(clickIterator.id, clickEvent.getTimestamp(), page, "");
            adapter.send("update", updateEvent.getPage(), Long.toString(updateEvent.getId()), objectMapper.writeValueAsString(updateEvent));
        }
    }

    protected static Properties getKafkaProps(String bootstrap){
        Properties props = new Properties();

        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrap);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getCanonicalName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getCanonicalName());


        return props;
    }

    private static Configuration getHdfsConf (String hdfsUri){
        Configuration configuration = new Configuration();

        configuration.set("fs.defaultFS", hdfsUri);

        return  configuration;
    }

    protected static Options createCLI(){
        Options options = new Options();

        Option kafkaOptn = Option.builder("kafka")
                .argName("bootstrap")
                .hasArg()
                .desc("Kafka bootstrap server")
                .build();

        Option hdfsUri = Option.builder("hdfs")
                .argName("uri")
                .hasArg()
                .desc("HDFS Uri")
                .build();

//        Option topicOptn = Option.builder("topic")
//                .argName("topic")
//                .hasArg()
//                .desc("Kafka input topic")
//                .build();

        Option delayOptn = Option.builder("delay")
                .argName("count")
                .hasArg()
                .desc("Insert delay after count events")
                .build();

        Option delayLengthOptn = Option.builder("length")
                .argName("delayLength")
                .hasArg()
                .desc("Length of delay after count events [ms]")
                .build();



        options.addOption(kafkaOptn);
        options.addOption(hdfsUri);
//        options.addOption(topicOptn);
        options.addOption(delayOptn);
        options.addOption(delayLengthOptn);


        return options;
    }


    static class ClickIterator  {

        private Map<String, Long> nextTimestampPerKey;
        private int nextPageIndex;
        public long id = 0L;

        ClickIterator() {
            nextTimestampPerKey = new HashMap<>();
            nextPageIndex = 0;
        }

        ClickEvent next() {
            String page = nextPage();
            id++;
            if (id == Long.MAX_VALUE) {
                id = Long.MIN_VALUE;
            }
            return new ClickEvent(id, nextTimestamp(page), page);
        }

        private Date nextTimestamp(String page) {
            long nextTimestamp = nextTimestampPerKey.getOrDefault(page, 1L);
//			nextTimestampPerKey.put(page, nextTimestamp + WINDOW_SIZE.toMilliseconds() / EVENTS_PER_WINDOW);
            nextTimestampPerKey.put(page, nextTimestamp + (WINDOW_SIZE.toMillis()  / EVENTS_PER_WINDOW ) );
            return new Date(nextTimestamp);
        }

        private String nextPage() {
            String nextPage = pages.get(nextPageIndex);
            if (nextPageIndex == pages.size() - 1) {
                nextPageIndex = 0;
            } else {
                nextPageIndex++;
            }
            return nextPage;
        }
    }

    static class UpdateIterator {
        private long lastUpdated = 30000L;
    }

}
